{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdn3219.leonardo.local\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/leonardo_work/IscrC_LAMPE/VLMs/PandaGPT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/leonardo_work/IscrC_LAMPE/vlm/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/leonardo_work/IscrC_LAMPE/vlm/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import ipdb\n",
    "import gradio as gr\n",
    "import mdtex2html\n",
    "from model.openllama import OpenLLAMAPEFTModel\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T17:15:29.206028Z",
     "start_time": "2024-12-06T17:15:29.189133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing visual encoder from ../pretrained_ckpt/imagebind_ckpt ...\n",
      "Visual encoder initialized.\n",
      "Initializing language decoder from ../pretrained_ckpt/vicuna_ckpt/7b_v0 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99f3b2ec61047bc907da343fd94cabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../pretrained_ckpt/vicuna_ckpt/7b_v0 and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33554432 || all params: 6771978240 || trainable%: 0.49548936530546206\n",
      "Language decoder initialized.\n",
      "[!] init the model over ...\n"
     ]
    }
   ],
   "source": [
    "# init the model\n",
    "args = {\n",
    "    'model': 'openllama_peft',\n",
    "    'imagebind_ckpt_path': '../pretrained_ckpt/imagebind_ckpt',\n",
    "    'vicuna_ckpt_path': '../pretrained_ckpt/vicuna_ckpt/7b_v0',\n",
    "    'delta_ckpt_path': '../pretrained_ckpt/pandagpt_ckpt/7b/pytorch_model.pt',\n",
    "    'stage': 2,\n",
    "    'max_tgt_len': 128,\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.1,\n",
    "}\n",
    "model = OpenLLAMAPEFTModel(**args)\n",
    "delta_ckpt = torch.load(args['delta_ckpt_path'], map_location=torch.device('cpu'))\n",
    "model.load_state_dict(delta_ckpt, strict=False)\n",
    "model = model.eval().half().cuda()\n",
    "print(f'[!] init the model over ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(self, y):\n",
    "    if y is None:\n",
    "        return []\n",
    "    for i, (message, response) in enumerate(y):\n",
    "        y[i] = (\n",
    "            None if message is None else mdtex2html.convert((message)),\n",
    "            None if response is None else mdtex2html.convert(response),\n",
    "        )\n",
    "    return y\n",
    "\n",
    "\n",
    "gr.Chatbot.postprocess = postprocess\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def re_predict(\n",
    "    input, \n",
    "    image_path, \n",
    "    audio_path, \n",
    "    video_path, \n",
    "    thermal_path, \n",
    "    chatbot, \n",
    "    max_length, \n",
    "    top_p, \n",
    "    temperature, \n",
    "    history, \n",
    "    modality_cache, \n",
    "):\n",
    "    # drop the latest query and answers and generate again\n",
    "    q, a = history.pop()\n",
    "    chatbot.pop()\n",
    "    return predict(q, image_path, audio_path, video_path, thermal_path, chatbot, max_length, top_p, temperature, history, modality_cache)\n",
    "\n",
    "\n",
    "def predict(\n",
    "    input, \n",
    "    image_path, \n",
    "    audio_path, \n",
    "    video_path, \n",
    "    thermal_path, \n",
    "    chatbot, \n",
    "    max_length, \n",
    "    top_p, \n",
    "    temperature, \n",
    "    history, \n",
    "    modality_cache, \n",
    "):\n",
    "    if image_path is None and audio_path is None and video_path is None and thermal_path is None:\n",
    "        return [(input, \"There is no input data provided! Please upload your data and start the conversation.\")]\n",
    "    else:\n",
    "        print(f'[!] image path: {image_path}\\n[!] audio path: {audio_path}\\n[!] video path: {video_path}\\n[!] thermal path: {thermal_path}')\n",
    "\n",
    "    # prepare the prompt\n",
    "    prompt_text = ''\n",
    "    for idx, (q, a) in enumerate(history):\n",
    "        if idx == 0:\n",
    "            prompt_text += f'{q}\\n### Assistant: {a}\\n###'\n",
    "        else:\n",
    "            prompt_text += f' Human: {q}\\n### Assistant: {a}\\n###'\n",
    "    if len(history) == 0:\n",
    "        prompt_text += f'{input}'\n",
    "    else:\n",
    "        prompt_text += f' Human: {input}'\n",
    "\n",
    "    response = model.generate({\n",
    "        'prompt': prompt_text,\n",
    "        'image_paths': [image_path] if image_path else [],\n",
    "        'audio_paths': [audio_path] if audio_path else [],\n",
    "        'video_paths': [video_path] if video_path else [],\n",
    "        'thermal_paths': [thermal_path] if thermal_path else [],\n",
    "        'top_p': top_p,\n",
    "        'temperature': temperature,\n",
    "        'max_tgt_len': max_length,\n",
    "        'modality_embeds': modality_cache\n",
    "    })\n",
    "    chatbot.append((parse_text(input), parse_text(response)))\n",
    "    history.append((input, response))\n",
    "    return chatbot, history, modality_cache\n",
    "\n",
    "\n",
    "def reset_user_input():\n",
    "    return gr.update(value='')\n",
    "\n",
    "def reset_dialog():\n",
    "    return [], []\n",
    "\n",
    "def reset_state():\n",
    "    return None, None, None, None, [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1147230/689063542.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Gradio app layout\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1 align='center'>PandaGPT</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Upload components for image, audio, video, and thermal image inputs\n",
    "        image_path = gr.Image(type=\"filepath\", label=\"Image\")\n",
    "        audio_path = gr.Audio(type=\"filepath\", label=\"Audio\")\n",
    "        video_path = gr.Video(label=\"Video\")\n",
    "        thermal_path = gr.Image(type=\"filepath\", label=\"Thermal Image\")\n",
    "\n",
    "    # Chatbot interface\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    # User input section\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=3)\n",
    "        submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        resubmitBtn = gr.Button(\"Resubmit\", variant=\"primary\")\n",
    "        emptyBtn = gr.Button(\"Clear History\")\n",
    "\n",
    "    # Settings section for max length, top p, and temperature\n",
    "    with gr.Row():\n",
    "        max_length = gr.Slider(minimum=0, maximum=400, value=256, label=\"Maximum Length\")\n",
    "        top_p = gr.Slider(minimum=0, maximum=1, value=0.01, step=0.01, label=\"Top P\")\n",
    "        temperature = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.01, label=\"Temperature\")\n",
    "\n",
    "    # States to keep track of chatbot history and modality cache\n",
    "    history = gr.State([])\n",
    "    modality_cache = gr.State([])\n",
    "\n",
    "    # Define interactions\n",
    "    submitBtn.click(\n",
    "        fn=predict, \n",
    "        inputs=[user_input, image_path, audio_path, video_path, thermal_path, chatbot, max_length, top_p, temperature, history, modality_cache], \n",
    "        outputs=[chatbot, history, modality_cache],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    resubmitBtn.click(\n",
    "        fn=re_predict, \n",
    "        inputs=[user_input, image_path, audio_path, video_path, thermal_path, chatbot, max_length, top_p, temperature, history, modality_cache], \n",
    "        outputs=[chatbot, history, modality_cache],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Additional interactions\n",
    "    submitBtn.click(fn=reset_user_input, inputs=[], outputs=[user_input])\n",
    "    emptyBtn.click(fn=reset_state, inputs=[], outputs=[image_path, audio_path, video_path, thermal_path, chatbot, history, modality_cache])\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
