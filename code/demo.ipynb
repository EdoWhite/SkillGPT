{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdn1171.leonardo.local\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/leonardo_work/IscrC_LAMPE/VLMs/PandaGPT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmdtex2html\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenLLAMAPEFTModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m/leonardo_work/IscrC_LAMPE/VLMs/PandaGPT/code/model/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepSpeedAgent\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenLLAMAPEFTModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(args):\n\u001b[1;32m      5\u001b[0m     agent_name \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m][args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/leonardo_work/IscrC_LAMPE/VLMs/PandaGPT/code/model/openllama.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageBind\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageBind\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaForCausalLM\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StoppingCriteria, StoppingCriteriaList\n",
      "File \u001b[0;32m/leonardo_work/IscrC_LAMPE/VLMs/PandaGPT/code/model/ImageBind/data.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_preprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleTokenizer\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m pv_transforms\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConstantClipsPerVideoSampler\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoded_video\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncodedVideo\n",
      "File \u001b[0;32m/leonardo_work/IscrC_LAMPE/vlm/lib/python3.11/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrand_augment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m/leonardo_work/IscrC_LAMPE/vlm/lib/python3.11/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     _AUGMENTATION_MAX_LEVEL,\n\u001b[1;32m      8\u001b[0m     AugmentTransform,\n\u001b[1;32m      9\u001b[0m     _decreasing_int_to_arg,\n\u001b[1;32m     10\u001b[0m     _decreasing_to_arg,\n\u001b[1;32m     11\u001b[0m     _increasing_magnitude_to_arg,\n\u001b[1;32m     12\u001b[0m     _increasing_randomly_negate_to_arg,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[1;32m     17\u001b[0m _AUGMIX_LEVEL_TO_ARG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoContrast\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEqualize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjustSharpness\u001b[39m\u001b[38;5;124m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[1;32m     31\u001b[0m }\n",
      "File \u001b[0;32m/leonardo_work/IscrC_LAMPE/vlm/lib/python3.11/site-packages/pytorchvideo/transforms/augmentations.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF_t\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# MaximumÂ global magnitude used for video augmentation.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import ipdb\n",
    "import gradio as gr\n",
    "import mdtex2html\n",
    "from model.openllama import OpenLLAMAPEFTModel\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T17:15:29.206028Z",
     "start_time": "2024-12-06T17:15:29.189133Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenLLAMAPEFTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# init the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenllama_peft\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagebind_ckpt_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../pretrained_ckpt/imagebind_ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora_dropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mOpenLLAMAPEFTModel\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     14\u001b[0m delta_ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta_ckpt_path\u001b[39m\u001b[38;5;124m'\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(delta_ckpt, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OpenLLAMAPEFTModel' is not defined"
     ]
    }
   ],
   "source": [
    "# init the model\n",
    "args = {\n",
    "    'model': 'openllama_peft',\n",
    "    'imagebind_ckpt_path': '../pretrained_ckpt/imagebind_ckpt',\n",
    "    'vicuna_ckpt_path': '../pretrained_ckpt/vicuna_ckpt/7b_v0',\n",
    "    'delta_ckpt_path': '../pretrained_ckpt/pandagpt_ckpt/7b/pytorch_model.pt',\n",
    "    'stage': 2,\n",
    "    'max_tgt_len': 128,\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.1,\n",
    "}\n",
    "model = OpenLLAMAPEFTModel(**args)\n",
    "delta_ckpt = torch.load(args['delta_ckpt_path'], map_location=torch.device('cpu'))\n",
    "model.load_state_dict(delta_ckpt, strict=False)\n",
    "model = model.eval().half().cuda()\n",
    "print(f'[!] init the model over ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(self, y):\n",
    "    if y is None:\n",
    "        return []\n",
    "    for i, (message, response) in enumerate(y):\n",
    "        y[i] = (\n",
    "            None if message is None else mdtex2html.convert((message)),\n",
    "            None if response is None else mdtex2html.convert(response),\n",
    "        )\n",
    "    return y\n",
    "\n",
    "\n",
    "gr.Chatbot.postprocess = postprocess\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def re_predict(\n",
    "    input, \n",
    "    image_path, \n",
    "    audio_path, \n",
    "    video_path, \n",
    "    thermal_path, \n",
    "    chatbot, \n",
    "    max_length, \n",
    "    top_p, \n",
    "    temperature, \n",
    "    history, \n",
    "    modality_cache, \n",
    "):\n",
    "    # drop the latest query and answers and generate again\n",
    "    q, a = history.pop()\n",
    "    chatbot.pop()\n",
    "    return predict(q, image_path, audio_path, video_path, thermal_path, chatbot, max_length, top_p, temperature, history, modality_cache)\n",
    "\n",
    "\n",
    "def predict(\n",
    "    input, \n",
    "    image_path, \n",
    "    audio_path, \n",
    "    video_path, \n",
    "    thermal_path, \n",
    "    chatbot, \n",
    "    max_length, \n",
    "    top_p, \n",
    "    temperature, \n",
    "    history, \n",
    "    modality_cache, \n",
    "):\n",
    "    if image_path is None and audio_path is None and video_path is None and thermal_path is None:\n",
    "        return [(input, \"There is no input data provided! Please upload your data and start the conversation.\")]\n",
    "    else:\n",
    "        print(f'[!] image path: {image_path}\\n[!] audio path: {audio_path}\\n[!] video path: {video_path}\\n[!] thermal path: {thermal_path}')\n",
    "\n",
    "    # prepare the prompt\n",
    "    prompt_text = ''\n",
    "    for idx, (q, a) in enumerate(history):\n",
    "        if idx == 0:\n",
    "            prompt_text += f'{q}\\n### Assistant: {a}\\n###'\n",
    "        else:\n",
    "            prompt_text += f' Human: {q}\\n### Assistant: {a}\\n###'\n",
    "    if len(history) == 0:\n",
    "        prompt_text += f'{input}'\n",
    "    else:\n",
    "        prompt_text += f' Human: {input}'\n",
    "\n",
    "    response = model.generate({\n",
    "        'prompt': prompt_text,\n",
    "        'image_paths': [image_path] if image_path else [],\n",
    "        'audio_paths': [audio_path] if audio_path else [],\n",
    "        'video_paths': [video_path] if video_path else [],\n",
    "        'thermal_paths': [thermal_path] if thermal_path else [],\n",
    "        'top_p': top_p,\n",
    "        'temperature': temperature,\n",
    "        'max_tgt_len': max_length,\n",
    "        'modality_embeds': modality_cache\n",
    "    })\n",
    "    chatbot.append((parse_text(input), parse_text(response)))\n",
    "    history.append((input, response))\n",
    "    return chatbot, history, modality_cache\n",
    "\n",
    "\n",
    "def reset_user_input():\n",
    "    return gr.update(value='')\n",
    "\n",
    "def reset_dialog():\n",
    "    return [], []\n",
    "\n",
    "def reset_state():\n",
    "    return None, None, None, None, [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://01162eb248f5b64386.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://01162eb248f5b64386.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/routes.py\", line 394, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/blocks.py\", line 1078, in process_api\n",
      "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/blocks.py\", line 1012, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/tmp/ipykernel_3065856/441310331.py\", line 4, in postprocess\n",
      "    for i, (message, response) in enumerate(y):\n",
      "ValueError: too many values to unpack (expected 2)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/routes.py\", line 394, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/blocks.py\", line 1078, in process_api\n",
      "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/blocks.py\", line 1012, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/tmp/ipykernel_3065856/441310331.py\", line 5, in postprocess\n",
      "    y[i] = (\n",
      "TypeError: 'tuple' object does not support item assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] image path: /tmp/tmp7n3vmk2s.png\n",
      "[!] audio path: None\n",
      "[!] video path: None\n",
      "[!] thermal path: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/routes.py\", line 394, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/blocks.py\", line 1078, in process_api\n",
      "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
      "  File \"/home/clusterusers/edbianchi/vlm/lib/python3.8/site-packages/gradio/blocks.py\", line 1012, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/tmp/ipykernel_3065856/441310331.py\", line 4, in postprocess\n",
      "    for i, (message, response) in enumerate(y):\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] image path: None\n",
      "[!] audio path: None\n",
      "[!] video path: /tmp/72cd7a4976afbf8f62509a4177c6b3499754ed3a/DemoClimb.mp4\n",
      "[!] thermal path: None\n",
      "[!] image path: None\n",
      "[!] audio path: None\n",
      "[!] video path: /tmp/72cd7a4976afbf8f62509a4177c6b3499754ed3a/DemoClimb.mp4\n",
      "[!] thermal path: None\n",
      "[!] image path: None\n",
      "[!] audio path: None\n",
      "[!] video path: /tmp/72cd7a4976afbf8f62509a4177c6b3499754ed3a/DemoClimb.mp4\n",
      "[!] thermal path: None\n"
     ]
    }
   ],
   "source": [
    "# Define Gradio app layout\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1 align='center'>PandaGPT</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Upload components for image, audio, video, and thermal image inputs\n",
    "        image_path = gr.Image(type=\"filepath\", label=\"Image\")\n",
    "        audio_path = gr.Audio(type=\"filepath\", label=\"Audio\")\n",
    "        video_path = gr.Video(type='file', label=\"Video\")\n",
    "        thermal_path = gr.Image(type=\"filepath\", label=\"Thermal Image\")\n",
    "\n",
    "    # Chatbot interface\n",
    "    chatbot = gr.Chatbot().style(height=300)\n",
    "\n",
    "    # User input section\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=3).style(container=False)\n",
    "        submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        resubmitBtn = gr.Button(\"Resubmit\", variant=\"primary\")\n",
    "        emptyBtn = gr.Button(\"Clear History\")\n",
    "\n",
    "    # Settings section for max length, top p, and temperature\n",
    "    with gr.Row():\n",
    "        max_length = gr.Slider(minimum=0, maximum=400, value=256, label=\"Maximum Length\")\n",
    "        top_p = gr.Slider(minimum=0, maximum=1, value=0.01, step=0.01, label=\"Top P\")\n",
    "        temperature = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.01, label=\"Temperature\")\n",
    "\n",
    "    # States to keep track of chatbot history and modality cache\n",
    "    history = gr.State([])\n",
    "    modality_cache = gr.State([])\n",
    "\n",
    "    # Define interactions\n",
    "    submitBtn.click(\n",
    "        fn=predict, \n",
    "        inputs=[user_input, image_path, audio_path, video_path, thermal_path, chatbot, max_length, top_p, temperature, history, modality_cache], \n",
    "        outputs=[chatbot, history, modality_cache],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    resubmitBtn.click(\n",
    "        fn=re_predict, \n",
    "        inputs=[user_input, image_path, audio_path, video_path, thermal_path, chatbot, max_length, top_p, temperature, history, modality_cache], \n",
    "        outputs=[chatbot, history, modality_cache],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Additional interactions\n",
    "    submitBtn.click(fn=reset_user_input, inputs=[], outputs=[user_input])\n",
    "    emptyBtn.click(fn=reset_state, inputs=[], outputs=[image_path, audio_path, video_path, thermal_path, chatbot, history, modality_cache])\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
