{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from model.openllama import OpenLLAMAPEFTModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    args = {\n",
    "        'model': 'openllama_peft',\n",
    "        'imagebind_ckpt_path': '../pretrained_ckpt/imagebind_ckpt',\n",
    "        'vicuna_ckpt_path': '../pretrained_ckpt/vicuna_ckpt/7b_v0',\n",
    "        'delta_ckpt_path': '../pretrained_ckpt/pandagpt_ckpt/7b/pytorch_model.pt',\n",
    "        'stage': 2,\n",
    "        'max_tgt_len': 128,\n",
    "        'lora_r': 32,\n",
    "        'lora_alpha': 32,\n",
    "        'lora_dropout': 0.1,\n",
    "    }\n",
    "\n",
    "    model = OpenLLAMAPEFTModel(**args)\n",
    "    delta_ckpt = torch.load(args['delta_ckpt_path'], map_location=torch.device('cuda'))\n",
    "    model.load_state_dict(delta_ckpt, strict=False)\n",
    "    model = model.eval().half().cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, input_text, image_path=None, audio_path=None, video_path=None, thermal_path=None, max_length=256, top_p=0.9, temperature=1.0):\n",
    "    # Prepare the prompt\n",
    "    prompt_text = input_text\n",
    "\n",
    "    # Generate response using the model\n",
    "    response = model.generate({\n",
    "        'prompt': prompt_text,\n",
    "        'image_paths': [image_path] if image_path else [],\n",
    "        'audio_paths': [audio_path] if audio_path else [],\n",
    "        'video_paths': [video_path] if video_path else [],\n",
    "        'thermal_paths': [thermal_path] if thermal_path else [],\n",
    "        'top_p': top_p,\n",
    "        'temperature': temperature,\n",
    "        'max_tgt_len': max_length,\n",
    "        'modality_embeds': ([])  # Assuming no modality embeddings in the CLI contex\n",
    "    })\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing visual encoder from ../pretrained_ckpt/imagebind_ckpt ...\n",
      "Visual encoder initialized.\n",
      "Initializing language decoder from ../pretrained_ckpt/vicuna_ckpt/7b_v0 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c1fa5cec7c4364a5134da949ba08d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../pretrained_ckpt/vicuna_ckpt/7b_v0 and are newly initialized: ['model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33554432 || all params: 6771978240 || trainable%: 0.49548936530546206\n",
      "Language decoder initialized.\n",
      "[!] Model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "print(\"[!] Model initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: A possible technical suggestion for the climber in the video is to practice their bouldering skills on different types of rock formations or colorful holds to challenge themselves and enhance their overall climbing abilities. By climbing on various shapes, sizes, and materials of holds, the climber can develop greater control, balance, and adaptability while climbing. This will help improve their technique and confidence, ultimately making them a more skilled and competent climber.\n"
     ]
    }
   ],
   "source": [
    "# Generate a response based on the inputs\n",
    "response = generate_response(\n",
    "    model, \n",
    "    input_text=\"What technical suggestion can you give to the climber in the video?\", \n",
    "    image_path=None, \n",
    "    audio_path=None, \n",
    "    video_path=\"/leonardo_work/IscrC_LAMPE/VLMs/SkillGPT/code/assets/videos/DemoClimb.mp4\", \n",
    "    thermal_path=None, \n",
    "    max_length=256, \n",
    "    top_p=0.9, \n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(f\"Model Response: {parse_text(response)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
